{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a141f318-7bdf-4561-9971-a6ee2a528056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from small2DNet import small2DNet\n",
    "from util import add_color, colorize, colorize_gaussian, calculate_correct_loss\n",
    "from colorMNist import colorMNist\n",
    "import random\n",
    "import colorsys\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08620ec1-2b3d-4a7d-b517-35f6a736f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: ntahur (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\nisha\\Documents\\DSTY2\\Thesis\\mscnishad\\code\\wandb\\run-20220321_165544-2i2kisen</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntahur/my-test-project/runs/2i2kisen\" target=\"_blank\">upbeat-firefly-4</a></strong> to <a href=\"https://wandb.ai/ntahur/my-test-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/ntahur/my-test-project/runs/2i2kisen?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x220673ed520>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"my-test-project\", entity=\"ntahur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe29bfee-cc02-4679-b1f6-f9f515d0bda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Cavgsim-0</td><td>██▆▄▃▃▂▂▁▁</td></tr><tr><td>Cavgsim-1</td><td>▁▂▄███▇▇▆▆</td></tr><tr><td>Cavgsim-2</td><td>▁▁▂▆████▇▇</td></tr><tr><td>Cavgsim-3</td><td>▁▁▂▇▇█████</td></tr><tr><td>Cavgsim-4</td><td>███▆▄▃▂▂▁▁</td></tr><tr><td>Cavgsim-5</td><td>▁▁▂▃▅▅▅▆▇█</td></tr><tr><td>Cavgsim-6</td><td>▃▁▁▂▄▄▅▆▆█</td></tr><tr><td>Cavgsim-7</td><td>█▇▃▁▃▄▄▄▄▄</td></tr><tr><td>Uavgsim-0</td><td>██▇▆▂▁▁▁▂▂</td></tr><tr><td>Uavgsim-1</td><td>▁▁▁▂▆██▇▆▆</td></tr><tr><td>Uavgsim-2</td><td>▁▁▁▁▅█████</td></tr><tr><td>Uavgsim-3</td><td>▁▁▁▂▆█████</td></tr><tr><td>Uavgsim-4</td><td>███▇▅▅▄▄▂▁</td></tr><tr><td>Uavgsim-5</td><td>▇█▇▇▅▁▁▁▃▅</td></tr><tr><td>Uavgsim-6</td><td>█▇▅▄▅▄▂▁▁▁</td></tr><tr><td>Uavgsim-7</td><td>▅▅▄▄▁▂▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Cavgsim-0</td><td>0.00297</td></tr><tr><td>Cavgsim-1</td><td>-0.30165</td></tr><tr><td>Cavgsim-2</td><td>0.46951</td></tr><tr><td>Cavgsim-3</td><td>0.47226</td></tr><tr><td>Cavgsim-4</td><td>0.49132</td></tr><tr><td>Cavgsim-5</td><td>-0.20042</td></tr><tr><td>Cavgsim-6</td><td>0.43379</td></tr><tr><td>Cavgsim-7</td><td>-0.12649</td></tr><tr><td>Uavgsim-0</td><td>0.0326</td></tr><tr><td>Uavgsim-1</td><td>-0.28752</td></tr><tr><td>Uavgsim-2</td><td>0.60893</td></tr><tr><td>Uavgsim-3</td><td>0.46637</td></tr><tr><td>Uavgsim-4</td><td>0.48876</td></tr><tr><td>Uavgsim-5</td><td>-0.22475</td></tr><tr><td>Uavgsim-6</td><td>0.41692</td></tr><tr><td>Uavgsim-7</td><td>0.23468</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">upbeat-firefly-4</strong>: <a href=\"https://wandb.ai/ntahur/my-test-project/runs/2i2kisen\" target=\"_blank\">https://wandb.ai/ntahur/my-test-project/runs/2i2kisen</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220321_165544-2i2kisen\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26766a3-cbb0-46cb-88f7-b0443ab4d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\n",
    "  \"learning_rate\": 0.01,\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d358fa0-abcc-4a97-b375-f3d77dfa62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"loss\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1896a-8436-4079-b0ba-133d57f53ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"loss\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248d05e4-3a5c-4b76-bc4c-bdbc2266f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 1000 5000\n"
     ]
    }
   ],
   "source": [
    "# Load data from pickle file\n",
    "cmnist_train, cmnist_val, cmnist_test = pickle.load(open(\"custom_datasets/cmnist_colorless.pkl\", \"rb\"))\n",
    "print(len(cmnist_train), len(cmnist_val), len(cmnist_train) + len(cmnist_val))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = colorMNist(cmnist_train)\n",
    "val_dataset = colorMNist(cmnist_val)\n",
    "test_dataset = colorMNist(cmnist_test)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
    "                                               shuffle=True, num_workers = 0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32,\n",
    "                                               shuffle=True, num_workers = 0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32,\n",
    "                                               shuffle=False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91532c56-e6b3-4125-b944-137fab9a8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=True, download=False, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=False, download=False, transform=transform)\n",
    "\n",
    "# Create the 80/20 train/val split\n",
    "train_split = int(0.8 * len(trainset))\n",
    "# Train dataset\n",
    "train_dataset = torch.utils.data.Subset(trainset, [i for i in range(train_split)])\n",
    "# Validation dataset\n",
    "val_dataset = torch.utils.data.Subset(trainset, [i for i in range(train_split, len(trainset))])\n",
    "print(len(train_dataset), len(val_dataset), len(train_dataset) + len(val_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1066f8bc-12b7-4c2f-b3a9-423146b008c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers of the model\n",
    "model_layers = [8, 8, \"M\", 16,\"M\"]\n",
    "# model_layers = [24, 24, \"M\", 48,\"M\"]\n",
    "# Set seed\n",
    "torch.manual_seed(12)\n",
    "# Create model\n",
    "linear_neurons = 512\n",
    "model = small2DNet(model_layers, 16, linear_neurons)\n",
    "# Load file and save file\n",
    "lfile = \"3Deterministic2D\"\n",
    "sfile = \"Deterministic2D\"\n",
    "# Load model\n",
    "# model.load_state_dict(torch.load('model_saves/Triple2D/'+ lfile + '.pth'))\n",
    "# Put model on gpu\n",
    "model.cuda()\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664191-4f3f-498c-ad48-c5ab61ec36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights\n",
    "children = [x for x in model.children()]\n",
    "for x in children[0]:\n",
    "    for param in x.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24726f1-4ff2-48fd-b766-c8f7c56553bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 193.75it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 813.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 336.05it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 859.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 344.91it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 914.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 316.41it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 654.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 312.76it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 593.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 295.21it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 682.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 300.89it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 729.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 309.99it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 763.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 255.26it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 654.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:00<00:00, 212.07it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 509.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train acc and loss\t 0.10675 \t 2.3007213401794435\n",
      "Val acc and loss\t 0.119 \t 2.293550565838814\n",
      "Epoch 2\n",
      "Train acc and loss\t 0.20675 \t 2.2805986289978026\n",
      "Val acc and loss\t 0.446 \t 2.2195304483175278\n",
      "Epoch 3\n",
      "Train acc and loss\t 0.5285 \t 1.4642793869972228\n",
      "Val acc and loss\t 0.799 \t 0.654135218821466\n",
      "Epoch 4\n",
      "Train acc and loss\t 0.78025 \t 0.6742028083801269\n",
      "Val acc and loss\t 0.858 \t 0.42327648866921663\n",
      "Epoch 5\n",
      "Train acc and loss\t 0.85675 \t 0.4434449883103371\n",
      "Val acc and loss\t 0.889 \t 0.3223112856503576\n",
      "Epoch 6\n",
      "Train acc and loss\t 0.90075 \t 0.32634836953878404\n",
      "Val acc and loss\t 0.933 \t 0.2273154665599577\n",
      "Epoch 7\n",
      "Train acc and loss\t 0.91675 \t 0.26654954904317857\n",
      "Val acc and loss\t 0.921 \t 0.2821725237299688\n",
      "Epoch 8\n",
      "Train acc and loss\t 0.921 \t 0.23926885211467744\n",
      "Val acc and loss\t 0.934 \t 0.20293090969789773\n",
      "Epoch 9\n",
      "Train acc and loss\t 0.935 \t 0.20947843539714814\n",
      "Val acc and loss\t 0.948 \t 0.16851679602405056\n",
      "Epoch 10\n",
      "Train acc and loss\t 0.94575 \t 0.17777234359085559\n",
      "Val acc and loss\t 0.953 \t 0.1486894676927477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs to train\n",
    "epochs = 10\n",
    "\n",
    "# Placeholder variables to put training and validation accuracies and losses per epoch\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch\", epoch + 1, \"/\", epochs)\n",
    "    \n",
    "    # Update learning rate\n",
    "    # if (epoch + 1) % 3 == 0:\n",
    "    #     for g in optimizer.param_groups:\n",
    "    #         g['lr'] /= 10\n",
    "    \n",
    "    # if (epoch + 1) % 5 == 0:\n",
    "    children = [x for x in model.children()]\n",
    "\n",
    "    for i, x in enumerate(children[0][0].weight):\n",
    "        x0 = x[0][0].cpu().detach().numpy().flatten().reshape(1, -1)\n",
    "        x1 = x[0][1].cpu().detach().numpy().flatten().reshape(1, -1)\n",
    "        x2 = x[0][2].cpu().detach().numpy().flatten().reshape(1, -1)\n",
    "        # wandb.log({\"Usim01-\"+str(i): cosine_similarity(x0, x1)[0][0]})\n",
    "        # wandb.log({\"Usim02-\"+str(i): cosine_similarity(x0, x2)[0][0]})\n",
    "        # wandb.log({\"Usim12-\"+str(i): cosine_similarity(x1, x2)[0][0]})\n",
    "        wandb.log({\"Cavgsim-\"+str(i): (cosine_similarity(x0, x1)[0][0] + cosine_similarity(x0, x2)[0][0] + cosine_similarity(x1, x2)[0][0]) / 3})\n",
    "    \n",
    "    # Put model on training mode\n",
    "    model.train()\n",
    "    train_total_correct = 0\n",
    "    train_total_loss = []\n",
    "    \n",
    "    for (images, labels) in tqdm(train_dataloader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate number correct and loss in batch\n",
    "        correct, loss = calculate_correct_loss(model, loss_fn, images, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Step function\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update amount correct and loss with current batch\n",
    "        train_total_correct += correct\n",
    "        train_total_loss.append(loss.item())\n",
    "        \n",
    "    # Append epoch accuracy and loss\n",
    "    train_accuracies.append(train_total_correct / len(train_dataset))\n",
    "    train_losses.append(sum(train_total_loss) / len(train_total_loss))\n",
    "    \n",
    "    # Put model on evaluation mode\n",
    "    model.eval()\n",
    "    val_total_correct = 0\n",
    "    val_total_loss = []\n",
    "    \n",
    "    # Without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in tqdm(val_dataloader):\n",
    "        \n",
    "            # Calculate number correct and loss in batch\n",
    "            correct, loss = calculate_correct_loss(model, loss_fn, images, labels)\n",
    "\n",
    "            # Update amount correct and loss with current batch\n",
    "            val_total_correct += correct\n",
    "            val_total_loss.append(loss.item())\n",
    "\n",
    "    # Append epoch accuracy and loss\n",
    "    val_accuracies.append(val_total_correct / len(val_dataset))\n",
    "    val_losses.append(sum(val_total_loss) / len(val_total_loss))\n",
    "    \n",
    "\n",
    "# Print accuracies and losses per epoch\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch\", i + 1)\n",
    "    print(\"Train acc and loss\\t\", train_accuracies[i], \"\\t\", train_losses[i])\n",
    "    print(\"Val acc and loss\\t\", val_accuracies[i], \"\\t\", val_losses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bfbb4-684b-4a14-8313-1d112d2149f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fixed_trainvalAccs/' + sfile + '.txt', 'w') as f:\n",
    "    for i in range(epochs):\n",
    "        f.write(\"Epoch \" + str(i + 1) + \"\\n\")\n",
    "        f.write(\"Train acc and loss\\t\" + str(train_accuracies[i]) + \"\\t\" + str(train_losses[i]) + \"\\n\")\n",
    "        f.write(\"Val acc and loss\\t\" + str(val_accuracies[i]) + \"\\t\" + str(val_losses[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b243c4-3a17-4904-8ea6-34f505a67251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'fixed_model_saves/2D/' + sfile + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceefcc7-0d3b-49b0-b957-f7b58c023f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers of the model\n",
    "model_layers = [8, 8, \"M\", 16,\"M\"]\n",
    "# model_layers = [24, 24, \"M\", 48,\"M\"]\n",
    "# Create model\n",
    "model = small2DNet(model_layers, 16, linear_neurons)\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('fixed_model_saves/2D/'+ sfile + '.pth'))\n",
    "# Put model on gpu\n",
    "model.cuda()\n",
    "hi = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c4348-231e-412f-9368-d991121d3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "    \n",
    "wrong_dict = {}\n",
    "right_dict = {}\n",
    "\n",
    "for i in range(10):\n",
    "    wrong_dict[i] = {}\n",
    "    for j in range(10):\n",
    "        wrong_dict[i][j] = 0\n",
    "    right_dict[i] = 0\n",
    "    \n",
    "for it in range(1):\n",
    "    # Total and amount correct\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # File to write predicted labels\n",
    "    # with open(\"predicted_labels.txt\", \"w\") as f:\n",
    "    #     f.write(\"\\n\")\n",
    "\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in tqdm(test_dataloader):\n",
    "\n",
    "            # Put images\n",
    "            images = images.cuda()\n",
    "\n",
    "            # Predicted labels\n",
    "            preds = model(images)\n",
    "\n",
    "            # Top predictions per image\n",
    "            _, top_preds = torch.max(preds, 1)\n",
    "\n",
    "            # Predictions and images back on cpu\n",
    "            top_preds = top_preds.cpu()\n",
    "            images = images.cpu()\n",
    "            \n",
    "            # Check the predicted\n",
    "            for i in range(len(labels)):\n",
    "                if top_preds[i].item() == labels[i].item():\n",
    "                    right_dict[top_preds[i].item()] += 1\n",
    "                else:\n",
    "                    wrong_dict[labels[i].item()][top_preds[i].item()] += 1\n",
    "\n",
    "            # Amount of correct predictions\n",
    "            predictions = [top_preds[i].item() == labels[i].item() for i in range(len(labels))]\n",
    "            correct = np.sum(predictions)\n",
    "\n",
    "    #         # Show batch images\n",
    "    #         fig, axs = plt.subplots(4,8, figsize=(28, 28), facecolor='w', edgecolor='k')\n",
    "    #         fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    #         axs = axs.ravel()\n",
    "    #         for i in range(len(images)):\n",
    "    #             axs[i].imshow(images[i].permute(1, 2, 0))\n",
    "\n",
    "    #         break\n",
    "\n",
    "            # Update total correct and total images\n",
    "            test_correct += correct\n",
    "            test_total += len(images)\n",
    "\n",
    "\n",
    "    print(\"Correct\", test_correct, \"/\", test_total, \"Accuracy:\", test_correct / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377b119-73e7-4a96-9d14-1a6055b7a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in right_dict:\n",
    "    print(x, \":\", right_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05ec81-4b70-461b-a5f8-346b52858ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = []\n",
    "for i, x in enumerate(wrong_dict):\n",
    "    hm.append([])\n",
    "    for y in wrong_dict[x]:\n",
    "        hm[i].append(wrong_dict[x][y])\n",
    "    # print(x, \":\", wrong_dict[x])\n",
    "print(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea7dfd-4203-4d34-b356-56ac3c72f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hm, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60345ef-2b1c-4aa9-a94f-03e6b9ba5447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

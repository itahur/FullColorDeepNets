{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06fa807-5ff1-42fb-ac07-23978bcb4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from receptive_field import receptive_field, receptive_field_for_unit\n",
    "from small2DNet import small2DNet\n",
    "from small3DNet import small3DNet\n",
    "from colorMNist import colorMNist\n",
    "from util import add_color, colorize, colorize_gaussian, calculate_correct_loss\n",
    "import random\n",
    "import colorsys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc2e1d0-343e-49b6-b65e-efee0466fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = [8, 8, \"M\", 16,\"M\"]\n",
    "linear_neurons = 512\n",
    "model = small2DNet(model_layers, 16, linear_neurons)\n",
    "model.load_state_dict(torch.load('model_saves/2D/Gaussian2D_36.pth'))\n",
    "model.cuda()\n",
    "model = model.features\n",
    "\n",
    "cl_model = small2DNet(model_layers, 16, linear_neurons)\n",
    "cl_model.load_state_dict(torch.load('model_saves/2D/Gaussian2D_36.pth'))\n",
    "cl_model.cuda()\n",
    "cl_model = cl_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd953dbd-1046-4c68-9a11-a009df9587a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model[:2]\n",
    "cl_model = cl_model[:2]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e69dbe-d424-4d3a-853f-531324fee1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "cl_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "cmnist_train, cmnist_val, cmnist_test = pickle.load(open(\"custom_datasets/cmnist_deterministic.pkl\", \"rb\"))\n",
    "test_dataset = colorMNist(cmnist_test)\n",
    "cmnist_train, cmnist_val, colorless_test = pickle.load(open(\"custom_datasets/cmnist_colorless.pkl\", \"rb\"))\n",
    "cl_dataset = colorMNist(colorless_test)\n",
    "\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=False, download=False, transform=transform)\n",
    "# cl_dataset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=False, download=False, transform=cl_transform)\n",
    "\n",
    "mean = np.asarray([0.1307, 0.1307, 0.1307])\n",
    "std = np.asarray([0.3081, 0.3081, 0.3081])\n",
    "# mean = np.asarray([0.5, 0.5, 0.5])\n",
    "# std = np.asarray([0.5, 0.5, 0.5])\n",
    "mtype=len(model[0].kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2ad20e-5b67-4f28-a61a-6f8b1c8f6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_features(model, dataset, max_activations, batch_size=32, top_n=100, \n",
    "                        out_dir='./NFResults',mean=[0,0,0], std=[1,1,1], mtype=2):\n",
    "    \"\"\"\n",
    "    Generates neuron features for given model definition using given dataset.\n",
    "    :param model: Pytorch model definition.\n",
    "    :param dataset: Dataset used for generating NFs.\n",
    "    :param batch_size: Batch size used for predicting feature maps.\n",
    "    :top_n: Use top_n input patch activations for generating NF.\n",
    "    :out_dir: Directory where generated images are stored.\n",
    "    :mean: Dataset mean used for normalization in transform function.\n",
    "    :std: Dataset std used for normalization in transform function.\n",
    "    \n",
    "    :return: returns nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # make output directory of not exists\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # set model in eval mode\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Set model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                   shuffle=False, num_workers = 0)\n",
    "    mean = np.asarray(mean)\n",
    "    std = np.asarray(std)\n",
    "\n",
    "    # input shape (c,w,h)\n",
    "    in_shape = next(iter(dataloader))[0].unsqueeze(1).shape[1:] if mtype == 3 else next(iter(dataloader))[0].shape[1:]\n",
    "    # receptive field info for entire model\n",
    "    receptive_field_dict = receptive_field(model, in_shape)\n",
    "    # output layer info\n",
    "    output_layer_info = receptive_field_dict[str(list(receptive_field_dict.keys())[-2])]\n",
    "    # check if fm has group convs\n",
    "    fm_groups = output_layer_info['output_shape'][2] if len(output_layer_info['output_shape']) == 5 else 0\n",
    "    # number of filters in last fm\n",
    "    n_filters = output_layer_info['output_shape'][1]\n",
    "    \n",
    "    # Create placeholder for input patches\n",
    "    rf = int(output_layer_info['r'])\n",
    "    \n",
    "    if fm_groups > 0:\n",
    "        fm_im = np.zeros((top_n,n_filters,fm_groups,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters,fm_groups))\n",
    "    else:\n",
    "        fm_im = np.zeros((top_n,n_filters,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters))\n",
    "\n",
    "    # Calculate amount of padding needed for input visualization\n",
    "    # Get range for rf at position 0,0 in final feature map\n",
    "    rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (0,0))\n",
    "    pad_y = int(rf-(rf_range[0][1]-rf_range[0][0]))\n",
    "    pad_x = int(rf-(rf_range[1][1]-rf_range[1][0]))\n",
    "    \n",
    "    \n",
    "    # Print summary\n",
    "    print('Group Convolutions: \\t {}, {} elements'.format(fm_groups>0, fm_groups))\n",
    "    print('Number of filters: \\t {}'.format(n_filters))\n",
    "    print('Receptive field size: \\t {}'.format(rf))\n",
    "    print('RF range at (0,0): \\t {}'.format(rf_range))\n",
    "    print('Input padding (x,y): \\t {}, {}'.format(pad_x, pad_y))\n",
    "    print('==============================================================================')\n",
    "\n",
    "    # Iterate over all data samples to get input patch for highest neuron activation\n",
    "    # for each filter and transformation\n",
    "    with torch.no_grad():\n",
    "        ids = 0\n",
    "        for inputs, labels in tqdm(dataloader, total=len(dataloader), desc='Extracting input patches: '):\n",
    "            if torch.cuda.is_available():\n",
    "                model_inputs = inputs.cuda()\n",
    "            else:\n",
    "                model_inputs = inputs\n",
    "                \n",
    "            # Add extra dimension for the network\n",
    "            if mtype == 3:\n",
    "                model_inputs = model_inputs.unsqueeze(1)\n",
    "\n",
    "            # Predict feature map\n",
    "            fm = model(model_inputs)\n",
    "\n",
    "            # Convert inputs to numpy w,h,c for visualization\n",
    "            inputs = inputs.permute((0,2,3,1)).numpy()\n",
    "            # Unnormalize\n",
    "            inputs *= std[None,None,None,:]\n",
    "            inputs += mean[None,None,None,:]\n",
    "            inputs = np.clip(inputs,0,1)\n",
    "            # Pad inputs for visualization to compensate for padding in layers\n",
    "            inputs = np.pad(inputs, ((0,0),(pad_y,pad_y),(pad_x,pad_x),(0,0)), mode='constant')\n",
    "\n",
    "            # get batch shape\n",
    "            fm_shape = fm.shape\n",
    "            # if gconv: reshape groups into channels\n",
    "            if fm_groups > 0:\n",
    "                fm = fm.view((fm_shape[0],-1,fm_shape[3],fm_shape[4]))\n",
    "\n",
    "            # Get max values and locations of feature maps\n",
    "            # pool size = fm size = fm_shape[-1]\n",
    "            a, b = F.max_pool2d(fm, (fm_shape[-2],fm_shape[-1]), return_indices=True)\n",
    "\n",
    "            # if gconv: reshape groups back to own dimension\n",
    "            if fm_groups > 0:\n",
    "                a = a.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "                b = b.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "\n",
    "            a = a.cpu().numpy()\n",
    "            b = b.cpu().numpy()\n",
    "\n",
    "            # coordinates of max activations\n",
    "            x = b % fm.shape[-1]\n",
    "            y = b // fm.shape[-1]\n",
    "\n",
    "            # store weight and input patches for each max position\n",
    "            for i in range(inputs.shape[0]):\n",
    "                for j in range(n_filters):\n",
    "\n",
    "                    if fm_groups == 0:\n",
    "                        # check if weight is higher than current lowest weight\n",
    "                        if a[i,j] > np.min(fm_w[:,j]):\n",
    "                            # replace lowest weight by current weight\n",
    "                            m = np.argmin(fm_w[:,j])\n",
    "                            fm_w[m,j] = a[i,j]\n",
    "                            # save indices\n",
    "                            max_activations[j].append(ids)\n",
    "                            # store input patch\n",
    "                            rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j],x[i,j]), bound=False)\n",
    "                            fm_im[m,j,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "\n",
    "                    else:\n",
    "                        # loop over extra dimension for gconv\n",
    "                        for k in range(fm_groups):\n",
    "                            # check if weight is higher than current lowest weight\n",
    "                            if a[i,j,k] > np.min(fm_w[:,j,k]):\n",
    "                                # replace lowest weight by current weight\n",
    "                                m = np.argmin(fm_w[:,j,k])\n",
    "                                # store weight\n",
    "                                fm_w[m,j,k] = a[i,j,k]\n",
    "                                # save indices\n",
    "                                max_activations[k][j].append(ids)\n",
    "                                # store input patch\n",
    "                                rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j,k],x[i,j,k]), bound=False)\n",
    "                                fm_im[m,j,k,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "                ids += 1\n",
    "    # Calculate and save neuron feature for each filter and transformation\n",
    "    for i in tqdm(range(n_filters), total=n_filters, desc='Generating neuron features: '):\n",
    "        if fm_groups == 0:\n",
    "            w_sum = np.sum(fm_w[:,i])\n",
    "            if w_sum > 0:\n",
    "                # Sort patches in order of highest neuron activations\n",
    "                idx = np.argsort(fm_w[:,i])[::-1] # ::-1 for high to low sort\n",
    "                fm_w[:,i] = fm_w[idx,i]\n",
    "                fm_im[:,i,:,:,:] = fm_im[idx,i,:,:,:]\n",
    "\n",
    "                # Calculate neuron feature\n",
    "                fm_nfw = fm_w[:,i,None,None,None]/w_sum\n",
    "                nf = np.sum(fm_im[:,i,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                # Plot 19 highest activated patches\n",
    "                plt.figure(figsize=(40,2))\n",
    "                for j in range(19):\n",
    "                    plt.subplot(1,20,j+2)\n",
    "                    plt.title('{:.3f}'.format(fm_w[j,i]))\n",
    "                    plt.imshow(fm_im[j,i,:,:,:])\n",
    "                # Plot NF\n",
    "                plt.subplot(1,20,1)\n",
    "                plt.imshow(nf)\n",
    "                plt.title('NF')\n",
    "                _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "                plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "                # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "        else:\n",
    "            plt.figure(figsize=(40,6))\n",
    "            for k in range(fm_groups):\n",
    "                w_sum = np.sum(fm_w[:,i,k])\n",
    "                if w_sum > 0:\n",
    "                    # Sort patches in order of highest neuron activations\n",
    "                    idx = np.argsort(fm_w[:,i,k])[::-1] # ::-1 for high to low sort\n",
    "                    fm_w[:,i,k] = fm_w[idx,i,k]\n",
    "                    fm_im[:,i,k,:,:,:] = fm_im[idx,i,k,:,:,:]\n",
    "\n",
    "                    # Calculate neuron feature\n",
    "                    fm_nfw = fm_w[:,i,k,None,None,None]/w_sum\n",
    "                    nf = np.sum(fm_im[:,i,k,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                    # Plot 19 highest activated patches\n",
    "                    for j in range(19):\n",
    "                        plt.subplot(fm_groups,20,j+2+20*k)\n",
    "                        plt.title('{:.3f}'.format(fm_w[j,i,k]))\n",
    "                        plt.imshow(fm_im[j,i,k,:,:,:])\n",
    "                    # Plot NF\n",
    "                    plt.subplot(fm_groups,20,1+20*k)\n",
    "                    plt.imshow(nf)\n",
    "                    plt.title('NF')\n",
    "            _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "            plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "            # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Done!')\n",
    "    \n",
    "    return max_activations, fm_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71487cc1-7f7c-4a4c-a57d-5a2a34998223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorless_get_neuron_features(model, dataset, activation_indices, indices, batch_size=32, top_n=100, \n",
    "                        out_dir='./NFResults',mean=[0,0,0], std=[1,1,1], mtype=2):\n",
    "    \"\"\"\n",
    "    Generates neuron features for given model definition using given dataset.\n",
    "    :param model: Pytorch model definition.\n",
    "    :param dataset: Dataset used for generating NFs.\n",
    "    :param batch_size: Batch size used for predicting feature maps.\n",
    "    :top_n: Use top_n input patch activations for generating NF.\n",
    "    :out_dir: Directory where generated images are stored.\n",
    "    :mean: Dataset mean used for normalization in transform function.\n",
    "    :std: Dataset std used for normalization in transform function.\n",
    "    \n",
    "    :return: returns nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # make output directory of not exists\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # set model in eval mode\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Set model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                   shuffle=False, num_workers = 0)\n",
    "    mean = np.asarray(mean)\n",
    "    std = np.asarray(std)\n",
    "\n",
    "    # input shape (c,w,h)\n",
    "    in_shape = next(iter(dataloader))[0].unsqueeze(1).shape[1:] if mtype == 3 else next(iter(dataloader))[0].shape[1:]\n",
    "    # receptive field info for entire model\n",
    "    receptive_field_dict = receptive_field(model, in_shape)\n",
    "    # output layer info\n",
    "    output_layer_info = receptive_field_dict[str(list(receptive_field_dict.keys())[-2])]\n",
    "    # check if fm has group convs\n",
    "    fm_groups = output_layer_info['output_shape'][2] if len(output_layer_info['output_shape']) == 5 else 0\n",
    "    # number of filters in last fm\n",
    "    n_filters = output_layer_info['output_shape'][1]\n",
    "    \n",
    "    # Create placeholder for input patches\n",
    "    rf = int(output_layer_info['r'])\n",
    "    \n",
    "    if fm_groups > 0:\n",
    "        fm_im = np.zeros((top_n,n_filters,fm_groups,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters,fm_groups))\n",
    "    else:\n",
    "        fm_im = np.zeros((top_n,n_filters,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters))\n",
    "\n",
    "    # Calculate amount of padding needed for input visualization\n",
    "    # Get range for rf at position 0,0 in final feature map\n",
    "    rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (0,0))\n",
    "    pad_y = int(rf-(rf_range[0][1]-rf_range[0][0]))\n",
    "    pad_x = int(rf-(rf_range[1][1]-rf_range[1][0]))\n",
    "    \n",
    "    \n",
    "    # Print summary\n",
    "    print('Group Convolutions: \\t {}, {} elements'.format(fm_groups>0, fm_groups))\n",
    "    print('Number of filters: \\t {}'.format(n_filters))\n",
    "    print('Receptive field size: \\t {}'.format(rf))\n",
    "    print('RF range at (0,0): \\t {}'.format(rf_range))\n",
    "    print('Input padding (x,y): \\t {}, {}'.format(pad_x, pad_y))\n",
    "    print('==============================================================================')\n",
    "\n",
    "    # Iterate over all data samples to get input patch for highest neuron activation\n",
    "    # for each filter and transformation\n",
    "    with torch.no_grad():\n",
    "        ids = 0\n",
    "        for inputs, labels in tqdm(dataloader, total=len(dataloader), desc='Extracting input patches: '):\n",
    "            if torch.cuda.is_available():\n",
    "                model_inputs = inputs.cuda()\n",
    "            else:\n",
    "                model_inputs = inputs\n",
    "                \n",
    "            # Add extra dimension for the network\n",
    "            if mtype == 3:\n",
    "                model_inputs = model_inputs.unsqueeze(1)\n",
    "                \n",
    "            # Predict feature map\n",
    "            fm = model(model_inputs)\n",
    "\n",
    "            # Convert inputs to numpy w,h,c for visualization\n",
    "            inputs = inputs.permute((0,2,3,1)).numpy()\n",
    "            # Unnormalize\n",
    "            inputs *= std[None,None,None,:]\n",
    "            inputs += mean[None,None,None,:]\n",
    "            inputs = np.clip(inputs,0,1)\n",
    "            # Pad inputs for visualization to compensate for padding in layers\n",
    "            inputs = np.pad(inputs, ((0,0),(pad_y,pad_y),(pad_x,pad_x),(0,0)), mode='constant')\n",
    "\n",
    "            # get batch shape\n",
    "            fm_shape = fm.shape\n",
    "            # if gconv: reshape groups into channels\n",
    "            if fm_groups > 0:\n",
    "                fm = fm.view((fm_shape[0],-1,fm_shape[3],fm_shape[4]))\n",
    "\n",
    "            # Get max values and locations of feature maps\n",
    "            # pool size = fm size = fm_shape[-1]\n",
    "            a, b = F.max_pool2d(fm, (fm_shape[-2],fm_shape[-1]), return_indices=True)\n",
    "\n",
    "            # if gconv: reshape groups back to own dimension\n",
    "            if fm_groups > 0:\n",
    "                a = a.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "                b = b.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "\n",
    "            a = a.cpu().numpy()\n",
    "            b = b.cpu().numpy()\n",
    "\n",
    "            # coordinates of max activations\n",
    "            x = b % fm.shape[-1]\n",
    "            y = b // fm.shape[-1]\n",
    "\n",
    "            # store weight and input patches for each max position\n",
    "            for i in range(inputs.shape[0]):\n",
    "                for j in range(n_filters):\n",
    "\n",
    "                    if fm_groups == 0:\n",
    "                        # check if weight is higher than current lowest weight\n",
    "                        if ids in indices[j][-100:]:\n",
    "                        # if a[i,j] > np.min(fm_w[:,j]):\n",
    "                            # replace lowest weight by current weight\n",
    "                            m = np.argmin(fm_w[:,j])\n",
    "                            activation_indices[j].append(ids)\n",
    "                            fm_w[m,j] = a[i,j]\n",
    "                            # store input patch\n",
    "                            rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j],x[i,j]), bound=False)\n",
    "                            fm_im[m,j,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "\n",
    "                    else:\n",
    "                        # loop over extra dimension for gconv\n",
    "                        for k in range(fm_groups):\n",
    "                            # check if weight is higher than current lowest weight\n",
    "                            if ids in indices[k][j][-100:]:\n",
    "                            # if a[i,j,k] > np.min(fm_w[:,j,k]):\n",
    "                                # replace lowest weight by current weight\n",
    "                                m = np.argmin(fm_w[:,j,k])\n",
    "                                # save index\n",
    "                                activation_indices[k][j].append(ids)\n",
    "                                # store weight\n",
    "                                fm_w[m,j,k] = a[i,j,k]\n",
    "                                # store input patch\n",
    "                                rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j,k],x[i,j,k]), bound=False)\n",
    "                                fm_im[m,j,k,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "                ids += 1\n",
    "    # Calculate and save neuron feature for each filter and transformation\n",
    "    for i in tqdm(range(n_filters), total=n_filters, desc='Generating neuron features: '):\n",
    "        if fm_groups == 0:\n",
    "            w_sum = np.sum(fm_w[:,i])\n",
    "            if w_sum > 0:\n",
    "                # Sort patches in order of highest neuron activations\n",
    "                idx = np.argsort(fm_w[:,i])[::-1] # ::-1 for high to low sort\n",
    "                fm_w[:,i] = fm_w[idx,i]\n",
    "                fm_im[:,i,:,:,:] = fm_im[idx,i,:,:,:]\n",
    "\n",
    "                # Calculate neuron feature\n",
    "                fm_nfw = fm_w[:,i,None,None,None]/w_sum\n",
    "                nf = np.sum(fm_im[:,i,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                # Plot 19 highest activated patches\n",
    "                plt.figure(figsize=(40,2))\n",
    "                for j in range(19):\n",
    "                    plt.subplot(1,20,j+2)\n",
    "                    plt.title('{:.3f}'.format(fm_w[j,i]))\n",
    "                    plt.imshow(fm_im[j,i,:,:,:])\n",
    "                # Plot NF\n",
    "                plt.subplot(1,20,1)\n",
    "                plt.imshow(nf)\n",
    "                plt.title('NF')\n",
    "                _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "                plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "                # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "        else:\n",
    "            plt.figure(figsize=(40,6))\n",
    "            for k in range(fm_groups):\n",
    "                w_sum = np.sum(fm_w[:,i,k])\n",
    "                if w_sum > 0:\n",
    "                    # Sort patches in order of highest neuron activations\n",
    "                    idx = np.argsort(fm_w[:,i,k])[::-1] # ::-1 for high to low sort\n",
    "                    fm_w[:,i,k] = fm_w[idx,i,k]\n",
    "                    fm_im[:,i,k,:,:,:] = fm_im[idx,i,k,:,:,:]\n",
    "\n",
    "                    # Calculate neuron feature\n",
    "                    fm_nfw = fm_w[:,i,k,None,None,None]/w_sum\n",
    "                    nf = np.sum(fm_im[:,i,k,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                    # Plot 19 highest activated patches\n",
    "                    for j in range(19):\n",
    "                        plt.subplot(fm_groups,20,j+2+20*k)\n",
    "                        plt.title('{:.3f}'.format(fm_w[j,i,k]))\n",
    "                        plt.imshow(fm_im[j,i,k,:,:,:])\n",
    "                    # Plot NF\n",
    "                    plt.subplot(fm_groups,20,1+20*k)\n",
    "                    plt.imshow(nf)\n",
    "                    plt.title('NF')\n",
    "            _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "            plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "            # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Done!')\n",
    "    \n",
    "    return activation_indices, fm_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482b604c-62e3-4c42-a267-901fa63567c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size        start       jump      receptive_field \n",
      "==============================================================================\n",
      "        0                 [28, 28]        0.5        1.0             1.0 \n",
      "        1                 [28, 28]        0.5        1.0             3.0 \n",
      "        2                 [28, 28]        0.5        1.0             3.0 \n",
      "        3                 [28, 28]        0.5        1.0             5.0 \n",
      "        4                 [28, 28]        0.5        1.0             5.0 \n",
      "        5                 [14, 14]        1.0        2.0             6.0 \n",
      "        6                 [14, 14]        1.0        2.0            10.0 \n",
      "        7                 [14, 14]        1.0        2.0            10.0 \n",
      "        8                   [7, 7]        2.0        4.0            12.0 \n",
      "==============================================================================\n",
      "Group Convolutions: \t False, 0 elements\n",
      "Number of filters: \t 16\n",
      "Receptive field size: \t 12\n",
      "RF range at (0,0): \t [(0, 8), (0, 8)]\n",
      "Input padding (x,y): \t 4, 4\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting input patches: 100%|██████████| 313/313 [00:02<00:00, 125.48it/s]\n",
      "Generating neuron features: 100%|██████████| 16/16 [00:09<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size        start       jump      receptive_field \n",
      "==============================================================================\n",
      "        0                 [28, 28]        0.5        1.0             1.0 \n",
      "        1                 [28, 28]        0.5        1.0             3.0 \n",
      "        2                 [28, 28]        0.5        1.0             3.0 \n",
      "        3                 [28, 28]        0.5        1.0             5.0 \n",
      "        4                 [28, 28]        0.5        1.0             5.0 \n",
      "        5                 [14, 14]        1.0        2.0             6.0 \n",
      "        6                 [14, 14]        1.0        2.0            10.0 \n",
      "        7                 [14, 14]        1.0        2.0            10.0 \n",
      "        8                   [7, 7]        2.0        4.0            12.0 \n",
      "==============================================================================\n",
      "Group Convolutions: \t False, 0 elements\n",
      "Number of filters: \t 16\n",
      "Receptive field size: \t 12\n",
      "RF range at (0,0): \t [(0, 8), (0, 8)]\n",
      "Input padding (x,y): \t 4, 4\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting input patches: 100%|██████████| 313/313 [00:01<00:00, 255.36it/s]\n",
      "Generating neuron features: 100%|██████████| 16/16 [00:09<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size        start       jump      receptive_field \n",
      "==============================================================================\n",
      "        0                 [28, 28]        0.5        1.0             1.0 \n",
      "        1                 [28, 28]        0.5        1.0             3.0 \n",
      "        2                 [28, 28]        0.5        1.0             3.0 \n",
      "        3                 [28, 28]        0.5        1.0             5.0 \n",
      "        4                 [28, 28]        0.5        1.0             5.0 \n",
      "        5                 [14, 14]        1.0        2.0             6.0 \n",
      "        6                 [14, 14]        1.0        2.0            10.0 \n",
      "        7                 [14, 14]        1.0        2.0            10.0 \n",
      "        8                   [7, 7]        2.0        4.0            12.0 \n",
      "==============================================================================\n",
      "Group Convolutions: \t False, 0 elements\n",
      "Number of filters: \t 16\n",
      "Receptive field size: \t 12\n",
      "RF range at (0,0): \t [(0, 8), (0, 8)]\n",
      "Input padding (x,y): \t 4, 4\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting input patches: 100%|██████████| 313/313 [00:01<00:00, 243.09it/s]\n",
      "Generating neuron features: 100%|██████████| 16/16 [00:09<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2D        \n",
    "activations = []\n",
    "cl_activations = []\n",
    "c_a = []\n",
    "\n",
    "for i in range(16):\n",
    "    activations.append([])\n",
    "    cl_activations.append([])\n",
    "    c_a.append([])\n",
    "    \n",
    "max_activations, w = get_neuron_features(model, test_dataset, activations, batch_size=32, top_n=100, out_dir='./NFResults/Test/', mean=mean, std=std)\n",
    "cl_activations, cl_w = colorless_get_neuron_features(cl_model, cl_dataset, cl_activations, max_activations, batch_size=32, top_n=100, out_dir='./NFResults/Test2/', mean=mean, std=std)\n",
    "c_a, c_w = colorless_get_neuron_features(model, test_dataset, c_a, max_activations, batch_size=32, top_n=100, out_dir='./NFResults/Test3/', mean=mean, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15ad333-7b71-4285-a26d-639975976b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0682\n",
      "-0.4278\n",
      "-0.1989\n",
      "-0.4633\n",
      "0.5738\n",
      "0.3502\n",
      "-0.5424\n",
      "-0.4431\n",
      "-0.4586\n",
      "-0.5871\n",
      "-0.4328\n",
      "-0.3563\n",
      "-0.3549\n",
      "-0.3824\n",
      "-0.37\n",
      "-0.3239\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "    # print(np.sum(w[:, i]))\n",
    "    # print(np.sum(cl_w[:, i]))\n",
    "    # print(np.sum(c_w[:, i]))\n",
    "    # print(\"\\n\")\n",
    "    print(np.round(1 - (np.sum(cl_w[:, i])) / np.sum(c_w[:, i]), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "453e61d2-0fcf-4931-ad89-be4bacf062fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x1656 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [i for i in range(16)]\n",
    "# plt.scatter(x, [1 for _ in range(16)])\n",
    "det = [0.052, 0.3887, 0.5325, 0.1642, 0.1285, 0.1383, -0.0124, 0.2835, 0.3871, 0.2901, 0.3219, 0.0447, 0.1185, 0.1646, 0.2708, 0.1855]\n",
    "g12 = [0.1676, -0.0346, 0.135, 0.6002, 0.4851, 0.6013, 0.0581, 0.2239, 0.2758, 0.4697, 0.9987, 0.3131, 0.1633, 0.4893, 0.3296, 0.5152]\n",
    "g24 = [-0.4098,-0.3823,-0.4289,0.0477,0.0377,-0.1413,-0.3226,-0.2412,-0.4294,-0.3962,-0.1304,-0.1347,-0.1407,0.0264,-0.2193,0.0151]\n",
    "g36 = [0.0682,-0.4278,-0.1989,-0.4633,0.5738,0.3502,-0.5424,-0.4431,-0.4586,-0.5871,-0.4328,-0.3563,-0.3549,-0.3824,-0.37,-0.3239]\n",
    "g72 = [-0.4525,-0.4544,-0.5589,-0.2577,-0.3702,-0.6219,-0.2575,-0.528,-0.4723,-0.4467,-0.4302,-0.5693,-0.4056,-0.4565,-0.593,-0.5546]\n",
    "uni = [-0.5458,-0.603,-0.5255,-0.5129,-0.5921,-0.6054,-0.5594,-0.5266,-0.7553,-0.5832,-0.6556,-0.4862,-0.5958,-0.5757,-0.3821,-0.6509]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(x, det, label=\"deterministic\")\n",
    "plt.scatter(x, g12, label=\"gaussian 12\")\n",
    "plt.scatter(x, g24, label=\"gaussian 24\")\n",
    "plt.scatter(x, g36, label=\"gaussian 36\")\n",
    "plt.scatter(x, g72, label=\"gaussian 72\")\n",
    "plt.scatter(x, uni, label=\"uniform\")\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.figure(figsize=(30, 23))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c29dd-a4bd-4995-971c-04777da1998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D\n",
    "activations = []\n",
    "cl_activations = []\n",
    "c_a = []\n",
    "\n",
    "for j in range(3):\n",
    "    activations.append([])\n",
    "    cl_activations.append([])\n",
    "    c_a.append([])\n",
    "    for i in range(16):\n",
    "        activations[j].append([])\n",
    "        cl_activations[j].append([])\n",
    "        c_a[j].append([])\n",
    "        \n",
    "max_activations, w = get_neuron_features(model, test_dataset, activations, batch_size=32, top_n=100, out_dir='./NFResults/Test/', mean=mean, std=std, mtype=mtype)\n",
    "cl_activations, cl_w = colorless_get_neuron_features(cl_model, cl_dataset, cl_activations, max_activations, batch_size=32, top_n=100, out_dir='./NFResults/Test2/', mean=mean, std=std, mtype=mtype)\n",
    "c_a, c_w = colorless_get_neuron_features(model, test_dataset, c_a, max_activations, batch_size=32, top_n=100, out_dir='./NFResults/Test3/', mean=mean, std=std, mtype=mtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc5b39-9408-4573-8605-388d8f4861a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    avg = 0\n",
    "    for k in range(3):\n",
    "        # print(np.sum(w[:, i]))\n",
    "        # print(np.sum(cl_w[:, i]))\n",
    "        # print(np.sum(c_w[:, i]))\n",
    "        # print(np.round(1 - (np.sum(cl_w[:, i, k])) / np.sum(c_w[:, i, k]), 4))\n",
    "        avg += np.round(1 - (np.sum(cl_w[:, i, k])) / np.sum(c_w[:, i, k]), 4)\n",
    "    \n",
    "    print(np.round(avg/3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508cf00-b7a5-4885-bccc-46f043ff3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_3d = [0.2823, 0.2937, 0.3684, 0.5469, 0.076, 0.6448, 0.0707, 0.7025, 0.1377, 0.6089, 0.1217, 0.1245, 0.2913, 0.6147, 0.3888, 0.0625]\n",
    "csi_2d = [-0.0101, 0.1312, 0.003, 0.0722, 0.0102, -0.001, 0.1987, -0.0048, 0.0046, -0.0033, 0.3337, 0.2284, 0.1583, 0.0278, 0.147, 0.2228]\n",
    "\n",
    "total = 0\n",
    "for i in range(len(csi_2d)):\n",
    "    total += (csi_2d[i] - csi_3d[i])\n",
    "    \n",
    "print(np.round(total, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

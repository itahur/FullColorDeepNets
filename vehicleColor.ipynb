{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647a7007-93f8-4a9a-afb8-314c15cea112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "from PIL import  Image\n",
    "from torchvision.transforms import  ToTensor, Compose, Resize, CenterCrop\n",
    "import glob\n",
    "from small2DNet import small2DNet\n",
    "from small3DNet import small3DNet\n",
    "from vggm2DNet import vggm2DNet\n",
    "from vggm3DNet import vggm3DNet\n",
    "from tqdm import tqdm\n",
    "from util import calculate_correct_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb5e14a-2a7b-4c48-8039-7fa4d6c59d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['black', 'blue' , 'cyan' , 'gray' , 'green' , 'red' , 'white' , 'yellow']\n",
    "def decode_label(index):\n",
    "    return  labels[index]\n",
    "\n",
    "def encode_label_from_path(path):\n",
    "    for index,value in enumerate(labels):\n",
    "        if value in path:\n",
    "            return  index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf5f4796-cef4-4245-b8f1-886d4163677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/vehicle/color/'\n",
    "image_list = glob.glob(path + '**/*')\n",
    "class_list = [encode_label_from_path(item) for item in image_list]\n",
    "x_trainval, x_test, y_trainval, y_test = train_test_split(image_list, class_list,\n",
    "                                                        train_size= 0.9, stratify=class_list, shuffle=True, random_state=42)\n",
    "class_listtv = [encode_label_from_path(item) for item in x_trainval]\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_trainval, class_listtv,\n",
    "                                                        train_size= 0.8, stratify=class_listtv, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92aa7fb1-5d0a-445d-837c-5a28516669ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleColorDataset(Dataset):\n",
    "    def __init__(self, image_list, class_list, transforms = None):\n",
    "        self.transform = transforms\n",
    "        self.image_list = image_list\n",
    "        self.class_list = class_list\n",
    "        self.data_len = len(self.image_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_list[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.class_list[index]\n",
    "\n",
    "transforms=Compose([Resize(224), CenterCrop(224), ToTensor()])\n",
    "train_dataset = VehicleColorDataset(x_train, y_train, transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 32)\n",
    "val_dataset = VehicleColorDataset(x_train, y_train, transforms)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size = 32)\n",
    "test_dataset = VehicleColorDataset(x_test, y_test, transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8212ad19-bc95-4c33-a693-995e1112a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 2 for 2D, 3 for 3D\n",
    "model_type = 2\n",
    "# Layers of the model\n",
    "model_layers = [96, 256, 512, 512, 512] if model_type == 2 else [96, 96, 296, 296, 296]\n",
    "# Set seed\n",
    "torch.manual_seed(12)\n",
    "# Create model\n",
    "model = vggm2DNet(model_layers) if model_type == 2 else vggm3DNet(model_layers)\n",
    "# Load file and save file\n",
    "# lfile = \"Gaussian2D_12\"\n",
    "sfile = \"Vehicle\" + str(model_type) + \"D\"\n",
    "# Load model\n",
    "# model.load_state_dict(torch.load('model_saves/new_fair/'+ lfile + '.pth'))\n",
    "# Put model on gpu\n",
    "model.cuda()\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4908c575-b251-476b-9bcb-25e62d29a2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train | Epoch 0: 100%|██████████| 68/68 [01:26<00:00,  1.28s/batch, accuracy=30.6, loss=1.84]\n",
      "Test | Epoch 0: 100%|██████████| 68/68 [01:14<00:00,  1.09s/batch, accuracy=30.4, loss=1.71]\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        model.train()\n",
    "        train_epoch_loss = []\n",
    "        train_epoch_correct = 0\n",
    "        \n",
    "        for images, labels in tepoch:\n",
    "            tepoch.set_description(f\"Train | Epoch {epoch}\")\n",
    "            optimizer.zero_grad()\n",
    "            correct, loss = calculate_correct_loss(model, loss_fn, images, labels, model_type)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_epoch_loss.append(loss.detach().item())\n",
    "            train_epoch_correct += correct\n",
    "            \n",
    "            tepoch.set_postfix(loss=sum(train_epoch_loss)/len(train_epoch_loss), accuracy=correct/(len(train_epoch_loss)*32+len(images))*100)\n",
    "            \n",
    "        train_losses.append(sum(train_epoch_loss) / len(train_epoch_loss))\n",
    "        train_accs.append(train_epoch_correct / len(train_dataset))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with tqdm(val_dataloader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Val | Epoch {epoch}\")\n",
    "            val_epoch_loss = []\n",
    "            val_epoch_correct = 0\n",
    "            \n",
    "            for images, labels in tepoch:\n",
    "                correct, loss = calculate_correct_loss(model, loss_fn, images, labels)\n",
    "            \n",
    "                val_epoch_loss.append(loss.detach().item())\n",
    "                val_epoch_correct += correct\n",
    "                \n",
    "                tepoch.set_postfix(loss=sum(val_epoch_loss)/len(val_epoch_loss), accuracy=correct/(len(val_epoch_loss)*32+len(images))*100)\n",
    "            \n",
    "        val_losses.append(sum(val_epoch_loss) / len(val_epoch_loss))\n",
    "        val_accs.append(val_epoch_correct / len(val_dataset))\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "            tepoch.set_description(f\"Test | Epoch {epoch}\")\n",
    "            test_epoch_loss = []\n",
    "            test_epoch_correct = 0\n",
    "            for images, labels in tepoch:\n",
    "                correct, loss = calculate_correct_loss(model, loss_fn, images, labels, model_type)\n",
    "                \n",
    "                test_epoch_loss.append(loss.detach().item())\n",
    "                test_epoch_correct += correct\n",
    "                \n",
    "                tepoch.set_postfix(loss=sum(test_epoch_loss)/len(test_epoch_loss), accuracy=correct/(len(test_epoch_loss)*32+len(images))*100)\n",
    "                \n",
    "            test_losses.append(sum(test_epoch_loss) / len(test_epoch_loss))\n",
    "            test_accs.append(test_epoch_correct / len(test_dataset))\n",
    "    # logger.checkpoint(model)\n",
    "\n",
    "np.savetxt(\"plots/VGGM_\" + str(epochs) + \"E\" + sfile + \".txt\", [train_losses, train_accs, val_losses, val_accs, test_losses, test_accs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2110f-4528-4c26-aa17-e169a4a84e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model_saves/vehicleSave/VGGM_' + str(epochs) + 'E' + sfile + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a709e5-f429-4546-97d6-bfe7c77859e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses)\n",
    "print(train_accs)\n",
    "print(test_losses)\n",
    "print(test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fa404-334d-4242-83c8-edf1690ce4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

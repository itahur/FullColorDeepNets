{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a141f318-7bdf-4561-9971-a6ee2a528056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from small2DNet import small2DNet\n",
    "from util import add_color, colorize, colorize_gaussian, calculate_correct_loss\n",
    "from colorMNist import colorMNist\n",
    "import random\n",
    "import colorsys\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248d05e4-3a5c-4b76-bc4c-bdbc2266f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 200 1000\n"
     ]
    }
   ],
   "source": [
    "# MNIST DATASET\n",
    "\n",
    "# Load data from pickle file\n",
    "cmnist_train, cmnist_val, cmnist_test = pickle.load(open(\"custom_datasets/1k/cmnist_gaussian_12.pkl\", \"rb\"))\n",
    "print(len(cmnist_train), len(cmnist_val), len(cmnist_train) + len(cmnist_val))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = colorMNist(cmnist_train)\n",
    "val_dataset = colorMNist(cmnist_val)\n",
    "test_dataset = colorMNist(cmnist_test)\n",
    "\n",
    "# Dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
    "                                               shuffle=True, num_workers = 0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32,\n",
    "                                               shuffle=True, num_workers = 0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32,\n",
    "                                               shuffle=False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91532c56-e6b3-4125-b944-137fab9a8f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 DATASET\n",
    "\n",
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=True, download=False, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=False, download=False, transform=transform)\n",
    "\n",
    "# Create the 80/20 train/val split\n",
    "train_split = int(0.8 * len(trainset))\n",
    "# Train dataset\n",
    "train_dataset = torch.utils.data.Subset(trainset, [i for i in range(train_split)])\n",
    "# Validation dataset\n",
    "val_dataset = torch.utils.data.Subset(trainset, [i for i in range(train_split, len(trainset))])\n",
    "print(len(train_dataset), len(val_dataset), len(train_dataset) + len(val_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1066f8bc-12b7-4c2f-b3a9-423146b008c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers of the model\n",
    "model_layers = [28, \"M\", 55, \"M\", 111,\"M\"]\n",
    "# Set seed\n",
    "torch.manual_seed(2)\n",
    "# Create model\n",
    "model = small2DNet(model_layers, model_layers[-2], [1, 1])\n",
    "# Load file and save file\n",
    "# lfile = \"Gaussian2D_12\"\n",
    "sfile = \"Gaussian2D_uniform\"\n",
    "# Load model\n",
    "# model.load_state_dict(torch.load('model_saves/new_fair/'+ lfile + '.pth'))\n",
    "# Put model on gpu\n",
    "model.cuda()\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_dataloader), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e664191-4f3f-498c-ad48-c5ab61ec36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze weights\n",
    "children = [x for x in model.children()]\n",
    "for x in children[0]:\n",
    "    for param in x.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b24726f1-4ff2-48fd-b766-c8f7c56553bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:01<00:00, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train acc and loss\t 0.2075 \t 2.2739684772491455\n",
      "Val acc and loss\t 0.215 \t 2.1856272561209544\n",
      "Epoch 2\n",
      "Train acc and loss\t 0.39 \t 1.6707205581665039\n",
      "Val acc and loss\t 0.58 \t 1.103754264967782\n",
      "Epoch 3\n",
      "Train acc and loss\t 0.62875 \t 0.9203212308883667\n",
      "Val acc and loss\t 0.61 \t 0.9902970365115574\n",
      "Epoch 4\n",
      "Train acc and loss\t 0.71625 \t 0.7936684727668762\n",
      "Val acc and loss\t 0.68 \t 0.8487601450511387\n",
      "Epoch 5\n",
      "Train acc and loss\t 0.42875 \t 1.6682796931266786\n",
      "Val acc and loss\t 0.25 \t 1.811011825289045\n",
      "Epoch 6\n",
      "Train acc and loss\t 0.43375 \t 1.528413140773773\n",
      "Val acc and loss\t 0.545 \t 1.3772424118859428\n",
      "Epoch 7\n",
      "Train acc and loss\t 0.6475 \t 1.2193508434295655\n",
      "Val acc and loss\t 0.78 \t 0.7262568516390664\n",
      "Epoch 8\n",
      "Train acc and loss\t 0.81125 \t 0.6100793164968491\n",
      "Val acc and loss\t 0.865 \t 0.4110597925526755\n",
      "Epoch 9\n",
      "Train acc and loss\t 0.84875 \t 0.43641544044017794\n",
      "Val acc and loss\t 0.86 \t 0.30194943451455664\n",
      "Epoch 10\n",
      "Train acc and loss\t 0.91375 \t 0.326254115998745\n",
      "Val acc and loss\t 0.94 \t 0.21795056494218962\n",
      "Epoch 11\n",
      "Train acc and loss\t 0.9075 \t 0.2946993938088417\n",
      "Val acc and loss\t 0.9 \t 0.3236971167581422\n",
      "Epoch 12\n",
      "Train acc and loss\t 0.9375 \t 0.18024738997220993\n",
      "Val acc and loss\t 0.93 \t 0.20895535192851508\n",
      "Epoch 13\n",
      "Train acc and loss\t 0.96875 \t 0.09804934851825237\n",
      "Val acc and loss\t 0.97 \t 0.20735020296914236\n",
      "Epoch 14\n",
      "Train acc and loss\t 0.98375 \t 0.04198726990027353\n",
      "Val acc and loss\t 0.95 \t 0.2966133473174913\n",
      "Epoch 15\n",
      "Train acc and loss\t 0.99125 \t 0.02925774090923369\n",
      "Val acc and loss\t 0.975 \t 0.1392628333664366\n",
      "Epoch 16\n",
      "Train acc and loss\t 0.99875 \t 0.011573512158356607\n",
      "Val acc and loss\t 0.975 \t 0.1273398613349335\n",
      "Epoch 17\n",
      "Train acc and loss\t 0.99875 \t 0.005616608127020299\n",
      "Val acc and loss\t 0.98 \t 0.1388727282007624\n",
      "Epoch 18\n",
      "Train acc and loss\t 1.0 \t 0.0037599871517159044\n",
      "Val acc and loss\t 0.975 \t 0.13011201309771941\n",
      "Epoch 19\n",
      "Train acc and loss\t 1.0 \t 0.003228874335763976\n",
      "Val acc and loss\t 0.975 \t 0.12841025549486013\n",
      "Epoch 20\n",
      "Train acc and loss\t 1.0 \t 0.0030694296525325625\n",
      "Val acc and loss\t 0.975 \t 0.12830279444772583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs to train\n",
    "epochs = 20\n",
    "\n",
    "# Placeholder variables to put training and validation accuracies and losses per epoch\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs), total=epochs, desc='Training'):\n",
    "    # print(\"Epoch\", epoch + 1, \"/\", epochs)\n",
    "    \n",
    "    # Update learning rate\n",
    "    # if (epoch + 1) % 20 == 0:\n",
    "    #     for g in optimizer.param_groups:\n",
    "    #         g['lr'] /= 10\n",
    "    \n",
    "    # Put model on training mode\n",
    "    model.train()\n",
    "    train_total_correct = 0\n",
    "    train_total_loss = []\n",
    "    \n",
    "    for (images, labels) in train_dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate number correct and loss in batch\n",
    "        correct, loss = calculate_correct_loss(model, loss_fn, images, labels)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Step function\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        # print(scheduler.get_last_lr())\n",
    "        \n",
    "        # Update amount correct and loss with current batch\n",
    "        train_total_correct += correct\n",
    "        train_total_loss.append(loss.detach().item())\n",
    "        \n",
    "    # Append epoch accuracy and loss\n",
    "    train_accuracies.append(train_total_correct / len(train_dataset))\n",
    "    train_losses.append(sum(train_total_loss) / len(train_total_loss))\n",
    "    \n",
    "    # Put model on evaluation mode\n",
    "    model.eval()\n",
    "    val_total_correct = 0\n",
    "    val_total_loss = []\n",
    "    \n",
    "    # Without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in val_dataloader:\n",
    "        \n",
    "            # Calculate number correct and loss in batch\n",
    "            correct, loss = calculate_correct_loss(model, loss_fn, images, labels)\n",
    "\n",
    "            # Update amount correct and loss with current batch\n",
    "            val_total_correct += correct\n",
    "            val_total_loss.append(loss.detach().item())\n",
    "\n",
    "    # Append epoch accuracy and loss\n",
    "    val_accuracies.append(val_total_correct / len(val_dataset))\n",
    "    val_losses.append(sum(val_total_loss) / len(val_total_loss))\n",
    "    \n",
    "\n",
    "# Print accuracies and losses per epoch\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch\", i + 1)\n",
    "    print(\"Train acc and loss\\t\", train_accuracies[i], \"\\t\", train_losses[i])\n",
    "    print(\"Val acc and loss\\t\", val_accuracies[i], \"\\t\", val_losses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5b8bfbb4-684b-4a14-8313-1d112d2149f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trainvalAccs/' + sfile + '.txt', 'w') as f:\n",
    "    for i in range(epochs):\n",
    "        f.write(\"Epoch \" + str(i + 1) + \"\\n\")\n",
    "        f.write(\"Train acc and loss\\t\" + str(train_accuracies[i]) + \"\\t\" + str(train_losses[i]) + \"\\n\")\n",
    "        f.write(\"Val acc and loss\\t\" + str(val_accuracies[i]) + \"\\t\" + str(val_losses[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57b243c4-3a17-4904-8ea6-34f505a67251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'testsave/' + sfile + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ceefcc7-0d3b-49b0-b957-f7b58c023f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers of the model\n",
    "model_layers = [28, \"M\", 55, \"M\", 111,\"M\"]\n",
    "# Create model\n",
    "model = small2DNet(model_layers, model_layers[-2], linear_neurons)\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('testsave/'+ sfile + '.pth'))\n",
    "# Put model on gpu\n",
    "model.cuda()\n",
    "hi = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "473c4348-231e-412f-9368-d991121d3d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 486.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct 9206 / 10000 Accuracy: 0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "    \n",
    "wrong_dict = {}\n",
    "right_dict = {}\n",
    "\n",
    "for i in range(10):\n",
    "    wrong_dict[i] = {}\n",
    "    for j in range(10):\n",
    "        wrong_dict[i][j] = 0\n",
    "    right_dict[i] = 0\n",
    "    \n",
    "for it in range(1):\n",
    "    # Total and amount correct\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    # File to write predicted labels\n",
    "    # with open(\"predicted_labels.txt\", \"w\") as f:\n",
    "    #     f.write(\"\\n\")\n",
    "\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Without gradient calculation\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in tqdm(test_dataloader):\n",
    "\n",
    "            # Put images\n",
    "            images = images.cuda()\n",
    "\n",
    "            # Predicted labels\n",
    "            preds = model(images)\n",
    "\n",
    "            # Top predictions per image\n",
    "            _, top_preds = torch.max(preds, 1)\n",
    "\n",
    "            # Predictions and images back on cpu\n",
    "            top_preds = top_preds.cpu()\n",
    "            images = images.cpu()\n",
    "            \n",
    "            # Check the predicted\n",
    "            for i in range(len(labels)):\n",
    "                if top_preds[i].item() == labels[i].item():\n",
    "                    right_dict[top_preds[i].item()] += 1\n",
    "                else:\n",
    "                    wrong_dict[labels[i].item()][top_preds[i].item()] += 1\n",
    "\n",
    "            # Amount of correct predictions\n",
    "            predictions = [top_preds[i].item() == labels[i].item() for i in range(len(labels))]\n",
    "            correct = np.sum(predictions)\n",
    "\n",
    "    #         # Show batch images\n",
    "    #         fig, axs = plt.subplots(4,8, figsize=(28, 28), facecolor='w', edgecolor='k')\n",
    "    #         fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    #         axs = axs.ravel()\n",
    "    #         for i in range(len(images)):\n",
    "    #             axs[i].imshow(images[i].permute(1, 2, 0))\n",
    "\n",
    "    #         break\n",
    "\n",
    "            # Update total correct and total images\n",
    "            test_correct += correct\n",
    "            test_total += len(images)\n",
    "\n",
    "\n",
    "    print(\"Correct\", test_correct, \"/\", test_total, \"Accuracy:\", test_correct / test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377b119-73e7-4a96-9d14-1a6055b7a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in right_dict:\n",
    "    print(x, \":\", right_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05ec81-4b70-461b-a5f8-346b52858ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = []\n",
    "for i, x in enumerate(wrong_dict):\n",
    "    hm.append([])\n",
    "    for y in wrong_dict[x]:\n",
    "        hm[i].append(wrong_dict[x][y])\n",
    "    # print(x, \":\", wrong_dict[x])\n",
    "print(hm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea7dfd-4203-4d34-b356-56ac3c72f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(hm, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60345ef-2b1c-4aa9-a94f-03e6b9ba5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [-0.5428, -0.6003, -0.5327, -0.5052, -0.5986, -0.6092, -0.5588, -0.5108, -0.8328, -0.5751, -0.6494, -0.4935, -0.6055, -0.5924, -0.3553, -0.6655]\n",
    "arr2 = [-0.5557, -0.5512, -0.5554, -0.9261, -0.5436, 0.9913, -0.5477, -0.5554, -0.5321, -0.5379, -0.5537, -0.5324, -0.5434, -0.5588, -0.5326, -0.5602]\n",
    "\n",
    "print(np.round(sum(arr1) - sum(arr2), 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d261b-87a6-4977-a97a-60462dabf937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

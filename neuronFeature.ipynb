{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06fa807-5ff1-42fb-ac07-23978bcb4c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from receptive_field import receptive_field, receptive_field_for_unit\n",
    "from small2DNet import small2DNet\n",
    "from small3DNet import small3DNet\n",
    "from colorMNist import colorMNist\n",
    "from util import add_color, colorize, colorize_gaussian, calculate_correct_loss\n",
    "import random\n",
    "import colorsys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6606c3-5000-4bb7-b2c2-2448c3f90c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNist dataset\n",
    "cmnist_train, cmnist_val, cmnist_test = pickle.load(open(\"custom_datasets/cmnist_deterministic.pkl\", \"rb\"))\n",
    "test_dataset = colorMNist(cmnist_test)\n",
    "# CIFAR dataset\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='datasets/cifar10', train=False, download=False, transform=transform)\n",
    "\n",
    "model_layers = [8, 8, \"M\", 16,\"M\"]\n",
    "model = small2DNet(model_layers, 16)\n",
    "model.load_state_dict(torch.load('model_saves/2D/Deterministic2D.pth'))\n",
    "model.cuda()\n",
    "model = model.features\n",
    "\n",
    "mean = np.asarray([0.1307, 0.1307, 0.1307])\n",
    "std = np.asarray([0.3081, 0.3081, 0.3081])\n",
    "mtype=len(model[0].kernel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b6d93e-36fa-4e62-b3fd-4860a707df10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model[:5]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2ad20e-5b67-4f28-a61a-6f8b1c8f6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neuron_features(model, dataset, batch_size=32, top_n=100, \n",
    "                        out_dir='./NFResults', mean=[0,0,0], std=[1,1,1], mtype=2):\n",
    "    \"\"\"\n",
    "    Generates neuron features for given model definition using given dataset.\n",
    "    :param model: Pytorch model definition.\n",
    "    :param dataset: Dataset used for generating NFs.\n",
    "    :param batch_size: Batch size used for predicting feature maps.\n",
    "    :top_n: Use top_n input patch activations for generating NF.\n",
    "    :out_dir: Directory where generated images are stored.\n",
    "    :mean: Dataset mean used for normalization in transform function.\n",
    "    :std: Dataset std used for normalization in transform function.\n",
    "    \n",
    "    :return: returns nothing\n",
    "    \"\"\"\n",
    "\n",
    "    # make output directory of not exists\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    # set model in eval mode\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Set model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                   shuffle=False, num_workers = 0)\n",
    "    mean = np.asarray(mean)\n",
    "    std = np.asarray(std)\n",
    "\n",
    "    # input shape (c,w,h)\n",
    "    in_shape = next(iter(dataloader))[0].unsqueeze(1).shape[1:] if mtype == 3 else next(iter(dataloader))[0].shape[1:]\n",
    "    # receptive field info for entire model\n",
    "    receptive_field_dict = receptive_field(model, in_shape)\n",
    "    # output layer info\n",
    "    output_layer_info = receptive_field_dict[str(list(receptive_field_dict.keys())[-2])]\n",
    "    # check if fm has group convs\n",
    "    fm_groups = output_layer_info['output_shape'][2] if len(output_layer_info['output_shape']) == 5 else 0\n",
    "    # number of filters in last fm\n",
    "    n_filters = output_layer_info['output_shape'][1]\n",
    "    \n",
    "    # Create placeholder for input patches\n",
    "    rf = int(output_layer_info['r'])\n",
    "    \n",
    "    if fm_groups > 0:\n",
    "        fm_im = np.zeros((top_n,n_filters,fm_groups,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters,fm_groups))\n",
    "    else:\n",
    "        fm_im = np.zeros((top_n,n_filters,rf,rf,3))\n",
    "        fm_w = -1e5*np.ones((top_n,n_filters))\n",
    "\n",
    "    # Calculate amount of padding needed for input visualization\n",
    "    # Get range for rf at position 0,0 in final feature map\n",
    "    rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (0,0))\n",
    "    pad_y = int(rf-(rf_range[0][1]-rf_range[0][0]))\n",
    "    pad_x = int(rf-(rf_range[1][1]-rf_range[1][0]))\n",
    "    \n",
    "    \n",
    "    # Print summary\n",
    "    print('Group Convolutions: \\t {}, {} elements'.format(fm_groups>0, fm_groups))\n",
    "    print('Number of filters: \\t {}'.format(n_filters))\n",
    "    print('Receptive field size: \\t {}'.format(rf))\n",
    "    print('RF range at (0,0): \\t {}'.format(rf_range))\n",
    "    print('Input padding (x,y): \\t {}, {}'.format(pad_x, pad_y))\n",
    "    print('==============================================================================')\n",
    "\n",
    "    # Iterate over all data samples to get input patch for highest neuron activation\n",
    "    # for each filter and transformation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, total=len(dataloader), desc='Extracting input patches: '):\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model_inputs = inputs.cuda()\n",
    "            else:\n",
    "                model_inputs = inputs\n",
    "                \n",
    "            # Add extra dimension for the network\n",
    "            if mtype == 3:\n",
    "                model_inputs = model_inputs.unsqueeze(1)\n",
    "\n",
    "            # Predict feature map\n",
    "            fm = model(model_inputs)\n",
    "            \n",
    "            \n",
    "\n",
    "            # Convert inputs to numpy w,h,c for visualization\n",
    "            inputs = inputs.permute((0,2,3,1)).numpy()\n",
    "            # Unnormalize\n",
    "            inputs *= std[None,None,None,:]\n",
    "            inputs += mean[None,None,None,:]\n",
    "            inputs = np.clip(inputs,0,1)\n",
    "            # Pad inputs for visualization to compensate for padding in layers\n",
    "            inputs = np.pad(inputs, ((0,0),(pad_y,pad_y),(pad_x,pad_x),(0,0)), mode='constant')\n",
    "\n",
    "            # get batch shape\n",
    "            fm_shape = fm.shape\n",
    "            # if gconv: reshape groups into channels\n",
    "            if fm_groups > 0:\n",
    "                fm = fm.view((fm_shape[0],-1,fm_shape[3],fm_shape[4]))\n",
    "\n",
    "            # Get max values and locations of feature maps\n",
    "            # pool size = fm size = fm_shape[-1]\n",
    "            a, b = F.max_pool2d(fm, (fm_shape[-2],fm_shape[-1]), return_indices=True)\n",
    "\n",
    "            # if gconv: reshape groups back to own dimension\n",
    "            if fm_groups > 0:\n",
    "                a = a.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "                b = b.view((fm_shape[0],fm_shape[1],fm_shape[2]))\n",
    "\n",
    "            a = a.cpu().numpy()\n",
    "            b = b.cpu().numpy()\n",
    "\n",
    "            # coordinates of max activations\n",
    "            x = b % fm.shape[-1]\n",
    "            y = b // fm.shape[-1]\n",
    "\n",
    "            # store weight and input patches for each max position\n",
    "            for i in range(inputs.shape[0]):\n",
    "                for j in range(n_filters):\n",
    "\n",
    "                    if fm_groups == 0:\n",
    "                        # check if weight is higher than current lowest weight\n",
    "                        if a[i,j] > np.min(fm_w[:,j]):\n",
    "                            # replace lowest weight by current weight\n",
    "                            m = np.argmin(fm_w[:,j])\n",
    "                            fm_w[m,j] = a[i,j]\n",
    "                            # store input patch\n",
    "                            rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j],x[i,j]), bound=False)\n",
    "                            fm_im[m,j,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "\n",
    "                    else:\n",
    "                        # loop over extra dimension for gconv\n",
    "                        for k in range(fm_groups):\n",
    "                            # check if weight is higher than current lowest weight\n",
    "                            if a[i,j,k] > np.min(fm_w[:,j,k]):\n",
    "                                # replace lowest weight by current weight\n",
    "                                m = np.argmin(fm_w[:,j,k])\n",
    "                                # store weight\n",
    "                                fm_w[m,j,k] = a[i,j,k]\n",
    "                                # store input patch\n",
    "                                rf_range = receptive_field_for_unit(receptive_field_dict, str(list(receptive_field_dict.keys())[-2]), (y[i,j,k],x[i,j,k]), bound=False)\n",
    "                                fm_im[m,j,k,:,:,:] = inputs[i,rf_range[0][0]+pad_y:rf_range[0][1]+pad_y,rf_range[1][0]+pad_x:rf_range[1][1]+pad_x,:]\n",
    "\n",
    "    # Calculate and save neuron feature for each filter and transformation\n",
    "    for i in tqdm(range(n_filters), total=n_filters, desc='Generating neuron features: '):\n",
    "        if fm_groups == 0:\n",
    "            w_sum = np.sum(fm_w[:,i])\n",
    "            if w_sum > 0:\n",
    "                # Sort patches in order of highest neuron activations\n",
    "                idx = np.argsort(fm_w[:,i])[::-1] # ::-1 for high to low sort\n",
    "                fm_w[:,i] = fm_w[idx,i]\n",
    "                fm_im[:,i,:,:,:] = fm_im[idx,i,:,:,:]\n",
    "\n",
    "                # Calculate neuron feature\n",
    "                fm_nfw = fm_w[:,i,None,None,None]/w_sum\n",
    "                nf = np.sum(fm_im[:,i,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                # Plot 19 highest activated patches\n",
    "                plt.figure(figsize=(40,2))\n",
    "                for j in range(19):\n",
    "                    plt.subplot(1,20,j+2)\n",
    "                    plt.title('{:.3f}'.format(fm_w[j,i]))\n",
    "                    plt.imshow(fm_im[j,i,:,:,:])\n",
    "                # Plot NF\n",
    "                plt.subplot(1,20,1)\n",
    "                plt.imshow(nf)\n",
    "                plt.title('NF')\n",
    "                _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "                plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "                # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "        else:\n",
    "            plt.figure(figsize=(40,6))\n",
    "            for k in range(fm_groups):\n",
    "                w_sum = np.sum(fm_w[:,i,k])\n",
    "                if w_sum > 0:\n",
    "                    # Sort patches in order of highest neuron activations\n",
    "                    idx = np.argsort(fm_w[:,i,k])[::-1] # ::-1 for high to low sort\n",
    "                    fm_w[:,i,k] = fm_w[idx,i,k]\n",
    "                    fm_im[:,i,k,:,:,:] = fm_im[idx,i,k,:,:,:]\n",
    "\n",
    "                    # Calculate neuron feature\n",
    "                    fm_nfw = fm_w[:,i,k,None,None,None]/w_sum\n",
    "                    nf = np.sum(fm_im[:,i,k,:,:,:]*fm_nfw, axis=0)\n",
    "\n",
    "                    # Plot 19 highest activated patches\n",
    "                    for j in range(19):\n",
    "                        plt.subplot(fm_groups,20,j+2+20*k)\n",
    "                        plt.title('{:.3f}'.format(fm_w[j,i,k]))\n",
    "                        plt.imshow(fm_im[j,i,k,:,:,:])\n",
    "                    # Plot NF\n",
    "                    plt.subplot(fm_groups,20,1+20*k)\n",
    "                    plt.imshow(nf)\n",
    "                    plt.title('NF')\n",
    "            _=plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "            plt.savefig(os.path.join(out_dir,'f_{:02d}.png'.format(i)), bbox_inches='tight')\n",
    "            # plt.savefig(os.path.join(out_dir,'f_{:02d}.pdf'.format(i)), bbox_inches='tight')\n",
    "            plt.close()\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8406d4b5-560f-4b38-a0cf-cca9390fc368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "        Layer (type)    map size        start       jump      receptive_field \n",
      "==============================================================================\n",
      "        0                 [28, 28]        0.5        1.0             1.0 \n",
      "        1                 [28, 28]        0.5        1.0             3.0 \n",
      "        2                 [28, 28]        0.5        1.0             3.0 \n",
      "        3                 [28, 28]        0.5        1.0             5.0 \n",
      "        4                 [28, 28]        0.5        1.0             5.0 \n",
      "        5                 [14, 14]        1.0        2.0             6.0 \n",
      "==============================================================================\n",
      "Group Convolutions: \t False, 0 elements\n",
      "Number of filters: \t 8\n",
      "Receptive field size: \t 6\n",
      "RF range at (0,0): \t [(0, 4), (0, 4)]\n",
      "Input padding (x,y): \t 2, 2\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting input patches: 100%|██████████| 313/313 [00:02<00:00, 126.23it/s]\n",
      "Generating neuron features: 100%|██████████| 8/8 [00:04<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_neuron_features(model, test_dataset, batch_size=32, top_n = 100, out_dir='./NFResults/Temp/', mean=mean, std=std, mtype=mtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c5be0-fd66-4225-9f4e-45deb0c0908a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
